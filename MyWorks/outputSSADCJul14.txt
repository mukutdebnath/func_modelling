['resnet20_adc']
Dict ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (adc1): ADC()
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): LambdaLayer()
    )
    (1): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
    (2): BasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): LambdaLayer()
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl1): ADC()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (adcl2): ADC()
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=64, out_features=10, bias=True)
)
DataParallel(
  (module): ResNet(
    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (adc1): ADC()
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): LambdaLayer()
      )
      (1): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): LambdaLayer()
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl1): ADC()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (adcl2): ADC()
        (shortcut): Sequential()
      )
    )
    (linear): Linear(in_features=64, out_features=10, bias=True)
  )
)
ADC: ss, Bits: 7
Saving model as: model_resnet20_ss_adc.pt
Files already downloaded and verified
current lr 1.00000e-01
Epoch: [0][0/391]	Time 4.838 (4.838)	Data 0.233 (0.233)	Loss 2.5109 (2.5109)	Prec@1 15.625 (15.625)
Epoch: [0][50/391]	Time 0.441 (0.546)	Data 0.000 (0.005)	Loss 1.9452 (2.0869)	Prec@1 28.906 (23.514)
Epoch: [0][100/391]	Time 0.407 (0.480)	Data 0.000 (0.002)	Loss 1.9186 (2.0014)	Prec@1 31.250 (25.897)
Epoch: [0][150/391]	Time 0.438 (0.460)	Data 0.000 (0.002)	Loss 1.8379 (1.9324)	Prec@1 28.125 (28.353)
Epoch: [0][200/391]	Time 0.437 (0.452)	Data 0.000 (0.001)	Loss 1.8019 (1.8863)	Prec@1 32.031 (30.092)
Epoch: [0][250/391]	Time 0.431 (0.448)	Data 0.000 (0.001)	Loss 1.8040 (1.8480)	Prec@1 33.594 (31.493)
Epoch: [0][300/391]	Time 0.434 (0.446)	Data 0.000 (0.001)	Loss 1.5836 (1.8170)	Prec@1 34.375 (32.602)
Epoch: [0][350/391]	Time 0.440 (0.446)	Data 0.000 (0.001)	Loss 1.5293 (1.7893)	Prec@1 46.875 (33.667)
Test: [0/79]	Time 0.754 (0.754)	Loss 1.5734 (1.5734)	Prec@1 37.500 (37.500)
Test: [50/79]	Time 0.461 (0.467)	Loss 1.6985 (1.5903)	Prec@1 39.062 (42.831)
 * Prec@1 42.740
Best Accuracy:42.74
current lr 1.00000e-01
Epoch: [1][0/391]	Time 0.702 (0.702)	Data 0.240 (0.240)	Loss 1.5739 (1.5739)	Prec@1 41.406 (41.406)
Epoch: [1][50/391]	Time 0.459 (0.457)	Data 0.000 (0.005)	Loss 1.5881 (1.5837)	Prec@1 39.844 (41.406)
Epoch: [1][100/391]	Time 0.467 (0.458)	Data 0.000 (0.003)	Loss 1.3815 (1.5485)	Prec@1 50.781 (42.922)
Epoch: [1][150/391]	Time 0.473 (0.460)	Data 0.000 (0.002)	Loss 1.5856 (1.5287)	Prec@1 41.406 (43.693)
Epoch: [1][200/391]	Time 0.475 (0.462)	Data 0.000 (0.001)	Loss 1.4330 (1.5097)	Prec@1 50.000 (44.430)
Epoch: [1][250/391]	Time 0.480 (0.467)	Data 0.000 (0.001)	Loss 1.5576 (1.4900)	Prec@1 42.969 (45.138)
Epoch: [1][300/391]	Time 0.498 (0.471)	Data 0.000 (0.001)	Loss 1.2820 (1.4660)	Prec@1 56.250 (46.127)
Epoch: [1][350/391]	Time 0.490 (0.475)	Data 0.000 (0.001)	Loss 1.3556 (1.4478)	Prec@1 47.656 (46.937)
Test: [0/79]	Time 0.878 (0.878)	Loss 1.4369 (1.4369)	Prec@1 48.438 (48.438)
Test: [50/79]	Time 0.510 (0.522)	Loss 1.8108 (1.6932)	Prec@1 40.625 (44.638)
 * Prec@1 44.590
Best Accuracy:44.59
current lr 1.00000e-01
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Epoch: [8][0/391]	Time 0.932 (0.932)	Data 0.305 (0.305)	Loss 0.7094 (0.7094)	Prec@1 78.125 (78.125)
Epoch: [8][50/391]	Time 0.605 (0.615)	Data 0.000 (0.006)	Loss 0.6153 (0.7230)	Prec@1 82.031 (73.928)
Epoch: [8][100/391]	Time 0.611 (0.614)	Data 0.000 (0.003)	Loss 0.7576 (0.7151)	Prec@1 74.219 (74.683)
Epoch: [8][150/391]	Time 0.604 (0.614)	Data 0.000 (0.002)	Loss 0.7400 (0.7113)	Prec@1 75.781 (74.922)
Epoch: [8][200/391]	Time 0.613 (0.614)	Data 0.000 (0.002)	Loss 0.5459 (0.7126)	Prec@1 81.250 (74.876)
Epoch: [8][250/391]	Time 0.623 (0.614)	Data 0.000 (0.001)	Loss 0.8303 (0.7131)	Prec@1 71.875 (75.019)
Epoch: [8][300/391]	Time 0.615 (0.614)	Data 0.000 (0.001)	Loss 0.8442 (0.7171)	Prec@1 71.875 (74.930)
Epoch: [8][350/391]	Time 0.614 (0.614)	Data 0.000 (0.001)	Loss 0.6711 (0.7180)	Prec@1 76.562 (74.900)
Test: [0/79]	Time 0.891 (0.891)	Loss 0.4706 (0.4706)	Prec@1 81.250 (81.250)
Test: [50/79]	Time 0.641 (0.644)	Loss 0.8496 (0.6895)	Prec@1 70.312 (76.731)
 * Prec@1 76.740
Best Accuracy:76.74
current lr 1.00000e-01
Epoch: [9][0/391]	Time 0.899 (0.899)	Data 0.251 (0.251)	Loss 0.6374 (0.6374)	Prec@1 82.031 (82.031)
Epoch: [9][50/391]	Time 0.617 (0.618)	Data 0.000 (0.005)	Loss 0.8223 (0.6754)	Prec@1 71.875 (76.302)
Epoch: [9][100/391]	Time 0.608 (0.617)	Data 0.000 (0.003)	Loss 0.7688 (0.6908)	Prec@1 71.094 (75.866)
Epoch: [9][150/391]	Time 0.609 (0.615)	Data 0.000 (0.002)	Loss 0.6279 (0.6954)	Prec@1 76.562 (75.574)
Epoch: [9][200/391]	Time 0.603 (0.614)	Data 0.000 (0.001)	Loss 0.6854 (0.6900)	Prec@1 77.344 (75.719)
Epoch: [9][250/391]	Time 0.611 (0.614)	Data 0.000 (0.001)	Loss 0.6861 (0.6888)	Prec@1 69.531 (75.669)
Epoch: [9][300/391]	Time 0.618 (0.613)	Data 0.000 (0.001)	Loss 0.6681 (0.6885)	Prec@1 73.438 (75.750)
Epoch: [9][350/391]	Time 0.610 (0.613)	Data 0.000 (0.001)	Loss 0.6827 (0.6875)	Prec@1 77.344 (75.797)
Test: [0/79]	Time 0.969 (0.969)	Loss 0.5609 (0.5609)	Prec@1 82.812 (82.812)
Test: [50/79]	Time 0.641 (0.649)	Loss 0.6098 (0.6674)	Prec@1 82.812 (77.956)
 * Prec@1 78.210
Best Accuracy:78.21
current lr 1.00000e-01
